# 02_RAG_Chatbot_app.py
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter  # ë³€ê²½
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser  # ë³€ê²½
from langchain_core.runnables import RunnablePassthrough, RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory  # ë³€ê²½
from langchain_community.embeddings import HuggingFaceEmbeddings

# PDF ì²˜ë¦¬ í•¨ìˆ˜
@st.cache_resource
def process_pdf():
    loader = PyPDFLoader("./data/êµ­ë¯¼ì£¼íƒì±„ê¶Œ ìì£¼í•˜ëŠ” ì§ˆë¬¸(FAQ).pdf")
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    return text_splitter.split_documents(documents)

# ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
@st.cache_resource
def initialize_vectorstore():
    chunks = process_pdf()
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
    return Chroma.from_documents(chunks, embeddings)

# ì²´ì¸ ì´ˆê¸°í™”
@st.cache_resource
def initialize_chain():
    vectorstore = initialize_vectorstore()
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    template = """ë‹¹ì‹ ì€ êµ­ë¯¼ì£¼íƒì±„ê¶Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

    ì»¨í…ìŠ¤íŠ¸: {context}
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", template),
        ("placeholder", "{chat_history}"),
        ("human", "{question}")
    ])

    model = ChatOpenAI(model_name="qwen2.5-3b-instruct-q4_k_m.gguf", temperature=0.2, base_url="http://localhost:8002/v1", api_key="EMPTY")

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    base_chain = (
        RunnablePassthrough.assign(
            context=lambda x: format_docs(retriever.invoke(x["question"]))
        )
        | prompt
        | model
        | StrOutputParser()
    )

    return RunnableWithMessageHistory(
        base_chain,
        lambda session_id: ChatMessageHistory(),
        input_messages_key="question",
        history_messages_key="chat_history",
    )

# Streamlit UI
def main():
    st.set_page_config(page_title="êµ­ë¯¼ì£¼íƒì±„ê¶Œ ì±—ë´‡", page_icon="ğŸ ")
    st.title("ğŸ  êµ­ë¯¼ì£¼íƒì±„ê¶Œ AI ì–´ë“œë°”ì´ì €")
    st.caption("êµ­ë¯¼ì£¼íƒì±„ê¶Œ ìì£¼í•˜ëŠ” ì§ˆë¬¸(FAQ) ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("êµ­ë¯¼ì£¼íƒì±„ê¶Œ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # ì²´ì¸ ì´ˆê¸°í™”
        chain = initialize_chain()

        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                response = chain.invoke(
                    {"question": prompt},
                    {"configurable": {"session_id": "streamlit_session"}}
                )
                st.markdown(response)

        st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()