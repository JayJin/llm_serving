{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97ee1ff",
   "metadata": {},
   "source": [
    "MULTIMODAL RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3acf0c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tesseract : PDF 파일에서 문자열 추출하는 OCR(광학문자인식) 라이브러리\n",
    "# Tesseract 설치\n",
    "# 윈도우의 경우 깃헙\n",
    "# Tesseract : (https://github.com/UB-Mannheim/tesseract/wiki)\n",
    "# Poppler : (https://github.com/oschwartz10612/poppler-windows/releases)\n",
    "# Poppler는 설치파일 다운로드 후 폴더 경로(... /Library/bin/) 환경변수 추가 필요\n",
    "\n",
    "# - 참고 : https://tesseract-ocr.github.io/tessdoc/Installation.html\n",
    "\n",
    "# !sudo apt install tesseract-ocr\n",
    "# !sudo apt install libtesseract-dev\n",
    "# !sudo apt-get install poppler-utils\n",
    "\n",
    "# ! pip install -U langchain openai chromadb langchain-experimental langchain_openai nltk pydantic lxml matplotlib chromadb tiktoken\n",
    "# ! pip install pillow==11.1.0\n",
    "# ! pip install \"unstructured[all-docs]\"==0.17.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f16724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로\n",
    "fpath = './data/'\n",
    "fname = \"sample.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "883abdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\wind9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\wind9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8518ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "import os\n",
    "\n",
    "# PDF에서 요소 추출\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=os.path.join(fpath, fname),            # PDF 파일 경로\n",
    "    extract_images_in_pdf=True,                     # 이미지 추출 여부\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],   # 추출할 이미지 블록 유형\n",
    "    chunking_strategy=\"by_title\",                   # 청킹 전략   \n",
    "    extract_image_block_output_dir=fpath,           # 추출된 이미지 저장 경로\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f339ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트, 테이블 추출\n",
    "tables = []\n",
    "texts = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        tables.append(str(element))  # 테이블 요소 추가\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        texts.append(str(element))  # 텍스트 요소 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73effd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'전년 대비(42주) 2023년 구분 2023년 증감 2024년 (1. 1.∼12. 31.) (%) (1. 1.∼10. 19.) (1. 1.∼10. 21.) 630 (100.0) 673 (100) 전체 663 (100.0) △5.0% 남자 성별 569 (84.5) 526 (83.5) 560 (84.5) △6.1% 1.0% 104 (15.5) 여자 103 (15.5) 104 (16.5) △60.0% 2 ( 0.3) 5 ( 0.7) 5 ( 0.8) 연령 0-9세 20 ( 3.2) 31 ( 4.6) 30 ( 4.5) 10-19세 △33.3% 20-29세 200 (30.2) 209 (33.2) 4.5% 201 (29.9) 30-39세 △18.2% 110 (16.6) 111 (16.5) 90 (14.3) △7.7% 104 (15.7) 96 (15.2) 107 (15.9) 40-49세 118 (17.5) 115 (17.3) △13.9% 50-59세 99 (15.7) 73 (11.6) 60-69세'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c6dfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42주차 (10.13.~10.19.)\\n\\n| Suyztay | aygises zt | Bail\\n\\n주차\\n\\n국내발생 해외유입\\n\\n전체'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae41eb",
   "metadata": {},
   "source": [
    "텍스트 및 테이블 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4558c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 프롬프트 설정\n",
    "prompt_text = \"\"\"당신은 표와 텍스트를 요약하여 검색할 수 있도록 돕는 역할을 맡은 어시스턴트입니다.\n",
    "이 요약은 임베딩되어 원본 텍스트나 표 요소를 검색하는 데 사용될 것입니다.\n",
    "표 또는 텍스트에 대한 간결한 요약을 제공하여 검색에 최적화된 형태로 만들어 주세요. 표 또는 텍스트: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# 텍스트 요약 체인\n",
    "model = ChatOpenAI(\n",
    "    model=\"qwen2.5-3b-instruct-q4_k_m.gguf\",\n",
    "    base_url=\"http://localhost:8002/v1\",\n",
    "    api_key=\"EMPTY\",   # 실제로 사용되지 않음\n",
    "    temperature=0.2,\n",
    ")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "# {\"element\": lambda x: x} : 입력값을 element 키로 매핑\n",
    "\n",
    "\n",
    "# 제공된 텍스트에 대해 요약을 할 경우\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})       # max_concurrency : 동시 처리 수\n",
    "# summarize_chain.batch : 여러 입력값에 대해 비동기적으로 요약 수행\n",
    "\n",
    "\n",
    "# 요약을 원치 않을 경우\n",
    "# text_summaries = texts\n",
    "\n",
    "# 제공된 테이블에 적용\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2794bd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023년 대비 2024년까지의 증감률을 요약하면 다음과 같습니다:\\n\\n- 전체: △5.0%\\n- 남자 성별: △6.1%\\n- 여자: △60.0%\\n- 연령별:\\n  - 0-9세: △33.3%\\n  - 10-19세: △18.2%\\n  - 20-29세: △7.7%\\n  - 30-39세: △13.9%\\n  - 40-49세: △15.7%\\n  - 50-59세: △15.2%\\n  - 60-69세: △15.9%\\n\\n이 요약은 원본 데이터의 주요 특징을 간략하게 요약하고, 검색에 최적화된 형태로 제공되었습니다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41c9ad5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42주차 (10월 13일 ~ 10월 19일) 요약: 국내에서 발생한 코로나19 환자 수와 해외에서 유입된 환자 수를 요약한 데이터입니다. 표에는 \"국내발생 해외유입\" 항목이 전반적인 코로나19 발생 현황을 나타내고 있습니다.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c6ff1d",
   "metadata": {},
   "source": [
    "요약에 약 16분 가량(GTX 1060 6GB 기준) 걸렸지만, 더 알아보기 쉬운 내용으로 요약된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9674f0",
   "metadata": {},
   "source": [
    "이미지 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19afc7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "\n",
    "def encode_image(image_path) -> str:\n",
    "    # 이미지 base64 인코딩\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "# 이미지의 base64 인코딩을 저장하는 리스트\n",
    "img_base64_list = []\n",
    "\n",
    "# 이미지를 읽어 base64 인코딩 후 저장\n",
    "for img_file in sorted(os.listdir(fpath)):              # sorted : 파일 이름 순서대로 정렬\n",
    "    if img_file.endswith('.jpg'):                       # endswith : 특정 확장자 파일만 선택\n",
    "        img_path = os.path.join(fpath, img_file)        #  이미지 파일 경로\n",
    "        base64_image = encode_image(img_path)           # 이미지 base64 인코딩 : base64로 인코딩하는 이유는 텍스트 기반 시스템에서 이미지를 쉽게 전송하고 저장하기 위함\n",
    "        img_base64_list.append(base64_image)            # 인코딩된 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1834152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_base64_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef73cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 리사이즈 및 재인코딩 함수\n",
    "\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def resize_image_base64(\n",
    "    img_base64: str,\n",
    "    max_size: int = 512,\n",
    "    quality: int = 85,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    base64 이미지를 받아서\n",
    "    - 최대 변을 max_size로 축소\n",
    "    - JPEG로 재인코딩\n",
    "    - base64 문자열 반환\n",
    "    \"\"\"\n",
    "    img_bytes = base64.b64decode(img_base64)\n",
    "    img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "    # 비율 유지하면서 축소\n",
    "    img.thumbnail((max_size, max_size))\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=\"JPEG\", quality=quality, optimize=True)\n",
    "\n",
    "    return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b325180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def image_summarize(img_base64: str) -> str:\n",
    "    # 이미지 리사이즈 및 재인코딩\n",
    "    img_base64 = resize_image_base64(img_base64, max_size=512)\n",
    "    \n",
    "    # 이미지 요약\n",
    "    chat = ChatOpenAI(\n",
    "    model=\"qwen2.5-vl-3b-instruct-q5_k_m.gguf\",\n",
    "    base_url=\"http://localhost:8003/v1\",\n",
    "    api_key=\"EMPTY\",   # 실제로 사용되지 않음\n",
    "    temperature=0.2,\n",
    "    )\n",
    "    prompt = \"\"\"\n",
    "    당신은 이미지를 요약하여 검색을 위해 사용할 수 있도록 돕는 어시스턴트입니다.\n",
    "    이 요약은 임베딩되어 원본 이미지를 검색하는 데 사용됩니다.\n",
    "    이미지 검색에 최적화된 간결한 요약을 작성하세요.\n",
    "    \"\"\"\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_base64}\"\n",
    "                        },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac1e51ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 요약을 저장하는 리스트\n",
    "image_summaries = []\n",
    "\n",
    "for img_base64 in img_base64_list:\n",
    "    image_summary = image_summarize(img_base64)     # 이미지 요약\n",
    "    image_summaries.append(image_summary)           # 요약된 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5687cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024년 말라리아 주간소식지 (42주차) (10.13.-10.19.) - KDCA (Korea Disease Control and Prevention Agency)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6f8f801",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:84\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\sentence_transformers\\__init__.py:15\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     13\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     14\u001b[0m )\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     CrossEncoder,\n\u001b[0;32m     17\u001b[0m     CrossEncoderModelCardData,\n\u001b[0;32m     18\u001b[0m     CrossEncoderTrainer,\n\u001b[0;32m     19\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:21\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     AutoConfig,\n\u001b[0;32m     23\u001b[0m     AutoModelForSequenceClassification,\n\u001b[0;32m     24\u001b[0m     AutoTokenizer,\n\u001b[0;32m     25\u001b[0m     PretrainedConfig,\n\u001b[0;32m     26\u001b[0m     PreTrainedModel,\n\u001b[0;32m     27\u001b[0m     PreTrainedTokenizer,\n\u001b[0;32m     28\u001b[0m     is_torch_npu_available,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2316\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2317\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2318\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2344\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.10.18-windows-x86_64-none\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:23\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     _BaseAutoBackboneClass,\n\u001b[0;32m     25\u001b[0m     _BaseAutoModelClass,\n\u001b[0;32m     26\u001b[0m     _LazyAutoMapping,\n\u001b[0;32m     27\u001b[0m     auto_class_update,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONFIG_MAPPING_NAMES\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:43\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[0;32m     46\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2316\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2317\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2318\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2344\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.10.18-windows-x86_64-none\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:120\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlignDevicesHook, add_hook_to_module\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Variable names used to hold the cache at generation time\u001b[39;00m\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\accelerate\\__init__.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.12.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbig_modeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     cpu_offload,\n\u001b[0;32m     19\u001b[0m     cpu_offload_with_hook,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     load_checkpoint_and_dispatch,\n\u001b[0;32m     25\u001b[0m )\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FP8BackendType\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbig_modeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _attach_context_parallel_hooks\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpointing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\accelerate\\big_modeling.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     AlignDevicesHook,\n\u001b[0;32m     27\u001b[0m     CpuOffload,\n\u001b[0;32m     28\u001b[0m     LayerwiseCastingHook,\n\u001b[0;32m     29\u001b[0m     UserCpuOffloadHook,\n\u001b[0;32m     30\u001b[0m     add_hook_to_module,\n\u001b[0;32m     31\u001b[0m     attach_align_device_hook,\n\u001b[0;32m     32\u001b[0m     attach_align_device_hook_on_blocks,\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     OffloadedWeightsLoader,\n\u001b[0;32m     36\u001b[0m     check_cuda_p2p_ib_support,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     retie_parameters,\n\u001b[0;32m     52\u001b[0m )\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\accelerate\\hooks.py:23\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PartialState\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     PrefixedDataset,\n\u001b[0;32m     25\u001b[0m     find_device,\n\u001b[0;32m     26\u001b[0m     named_module_tensors,\n\u001b[0;32m     27\u001b[0m     send_to_device,\n\u001b[0;32m     28\u001b[0m     set_module_tensor_to_device,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     is_mlu_available,\n\u001b[0;32m     32\u001b[0m     is_musa_available,\n\u001b[0;32m     33\u001b[0m     is_npu_available,\n\u001b[0;32m     34\u001b[0m )\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\accelerate\\utils\\__init__.py:226\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    216\u001b[0m         DeepSpeedEngineWrapper,\n\u001b[0;32m    217\u001b[0m         DeepSpeedOptimizerWrapper,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m         map_pytorch_optim_to_deepspeed,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbnb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m has_4bit_bnb_layers, load_and_quantize_model\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    228\u001b[0m     disable_fsdp_ram_efficient_loading,\n\u001b[0;32m    229\u001b[0m     enable_fsdp_ram_efficient_loading,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m     save_fsdp_optimizer,\n\u001b[0;32m    242\u001b[0m )\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\accelerate\\utils\\bnb.py:29\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     is_4bit_bnb_available,\n\u001b[0;32m     26\u001b[0m     is_8bit_bnb_available,\n\u001b[0;32m     27\u001b[0m )\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbig_modeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatch_model, init_empty_weights\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BnbQuantizationConfig\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'dispatch_model' from partially initialized module 'accelerate.big_modeling' (most likely due to a circular import) (d:\\llm_serving\\.venv\\lib\\site-packages\\accelerate\\big_modeling.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[1;32m---> 11\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBAAI/bge-m3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 분할한 텍스트들을 색인할 벡터 저장소\u001b[39;00m\n\u001b[0;32m     16\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m Chroma(collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_modal_rag\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m                      embedding_function\u001b[38;5;241m=\u001b[39membedding)\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:239\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     emit_warning()\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\llm_serving\\.venv\\lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:87\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer(\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, cache_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\n\u001b[0;32m     94\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`."
     ]
    }
   ],
   "source": [
    "# from langchain.retrievers import MultiVectorRetriever\n",
    "# from langchain_community.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.stores import InMemoryStore\n",
    "\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\"\n",
    "    )\n",
    "\n",
    "# 분할한 텍스트들을 색인할 벡터 저장소\n",
    "vectorstore = Chroma(collection_name=\"multi_modal_rag\",\n",
    "                     embedding_function=embedding)\n",
    "\n",
    "# 원본문서 저장을 위한 저장소 선언\n",
    "docstore = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# # 검색기\n",
    "# retriever = MultiVectorRetriever(\n",
    "#     vectorstore=vectorstore,\n",
    "#     docstore=docstore,\n",
    "#     id_key=id_key,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ac7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# 원본 텍스트 데이터 저장\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# 원본 테이블 데이터 저장\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# 원본 이미지(base64) 데이터 저장\n",
    "img_ids = [str(uuid.uuid4()) for _ in img_base64_list]\n",
    "retriever.docstore.mset(list(zip(img_ids, img_base64_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c352d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.document import Document\n",
    "\n",
    "# 텍스트 요약 벡터 저장\n",
    "summary_texts = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "\n",
    "# 테이블 요약 벡터 저장\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "\n",
    "# 이미지 요약 벡터 저장\n",
    "\n",
    "summary_img = [\n",
    "    Document(page_content=s, metadata={id_key: img_ids[i]})\n",
    "    for i, s in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9613156",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\n",
    "    \"말라리아 군집 사례는 어떤가요? \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00288df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b23ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64decode\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    # 이미지와 텍스트 데이터를 분리\n",
    "    b64 = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            b64decode(doc)\n",
    "            b64.append(doc)\n",
    "        except Exception as e:\n",
    "            text.append(doc)\n",
    "    return {\n",
    "        \"images\": b64,\n",
    "        \"texts\": text\n",
    "    }\n",
    "\n",
    "docs_by_type = split_image_text_types(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_by_type[\"images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85bdf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(docs_by_type[\"texts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d254fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_by_type[\"images\"][0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b38d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_by_type[\"texts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ef605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    # base64 이미지로 html 태그를 작성합니다\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "\n",
    "    # html 태그를 기반으로 이미지를 표기합니다\n",
    "    display(HTML(image_html))\n",
    "\n",
    "plt_img_base64(docs_by_type[\"images\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_by_type[\"texts\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f88a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def prompt_func(dict):\n",
    "    format_texts = \"\\n\".join(dict[\"context\"][\"texts\"])\n",
    "    text = f\"\"\"\n",
    "    다음 문맥에만 기반하여 질문에 답하세요. 문맥에는 텍스트, 표, 그리고 아래 이미지가 포함될 수 있습니다:\n",
    "    질문: {dict[\"question\"]}\n",
    "\n",
    "    텍스트와 표:\n",
    "    {format_texts}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = [\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": text},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{dict['context']['images'][0]}\"}},\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "# RAG 파이프라인\n",
    "chain = (\n",
    "        {\"context\": retriever | RunnableLambda(split_image_text_types), \"question\": RunnablePassthrough()}\n",
    "        | RunnableLambda(prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f454707",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\n",
    "    \"말라리아 군집 사례는 어떤가요?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_serving (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
